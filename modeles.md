---
layout: post
title: "Journée d'étude sur les modèles de langage"
permalink: 2021-modeles-fr

nav-menu: false

lang: fr
lang-ref: ev-modeles
---


## Présentation

Pour reprendre la [définition de Wikipedia](https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_langage) : "En traitement automatique des langues, un modèle de langage est un modèle statistique qui modélise la distribution de séquences de mots, et plus généralement de séquences de symboles discrets (lettres, phonèmes, mots), dans une langue naturelle. Un modèle de langage peut par exemple prédire le mot suivant une séquence de mots. BERT et GPT-3 sont des modèles de langage".

Ces modèles, aujourd'hui omniprésents en traitement automatisé du langage, posent de nombreux problèmes : ils incluent de nombreux biais (biais de représentation liés au  genre, à l'âge ou à l'origine par exemple), ils nécessitent de grandes masses de données et ne sont donc disponibles que pour quelques langues. Enfin, ils sont très lourds à entraîner et ont donc un coût environnemental très important.  

## Programme

Les questions liées aux modèles de langage ont été abordées lors de la journée du groupe "Ethique et Intelligence Artificielle" organisée en ligne le mercredi 1er juillet 2021. Le programme était le suivant :

* Benoît Sagot (INRIA) : Les modèles de langue neuronaux : biais de représentativité et de représentation  (la présentation sera mise en ligne dès sa réception)
* Karine Gentelet (UQO) : [Le numérique (et l'IA): un outil pertinent dans les stratégies politique/identitaire des Peuples autochtones du Canada](https://ais-initiative.github.io/gentelet-modeles.pdf)
* Daniel Andler (IJN, IUF) : [Qui parle ?](https://ais-initiative.github.io/andler-gpt3.pdf)

La réunion a eu lieu en ligne, sur Zoom.

### Références annexes

Notons, sur les mêmes questions, la [réflexion](https://hai.stanford.edu/news/reflections-foundation-models) menée au [Human Centered Artificial Intelligence Centre](https://hai.stanford.edu/) de Stanford, qui a produit un [rapport](https://arxiv.org/abs/2108.07258) et a organisé un [workshop](https://crfm.stanford.edu/workshop.html) dédié à ces questions. Le rapport contient une bonne synthèse des questions soulevées.

Voir aussi, à titre d'introduction, plusieurs articles de Thierry Poibeau paru dans TheConversation :
* [Quand l’IA prend la parole : des prouesses aux dangers](https://theconversation.com/quand-lia-prend-la-parole-des-prouesses-aux-dangers-153495),
* [Peut-on détecter des fake news automatiquement ?](https://theconversation.com/peut-on-detecter-des-fake-news-automatiquement-160398)
* et [Emploi, sécurité, justice : d’où viennent les « biais » des IA et peut-on les éviter ?](https://theconversation.com/emploi-securite-justice-dou-viennent-les-biais-des-ia-et-peut-on-les-eviter-154579).

<a href="evenements" class="button special icon fa-arrow-left">Retour aux évènements</a>
<a href="#" class="button special icon fa-arrow-up">Retour en haut de page</a>
